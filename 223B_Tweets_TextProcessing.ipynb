{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import (wordnet, stopwords)\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import (TfidfVectorizer, CountVectorizer)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import *\n",
    "from bisect import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Functions to parse the time\n",
    "def parse_datetime(raw_datetime, dtformat):\n",
    "    \"\"\" Creates a datetime object from the inputted string using the datetime library.\n",
    "    \"\"\"\n",
    "    if len(raw_datetime)<5:\n",
    "        return np.nan, 365, np.nan, np.nan    \n",
    "    datetime_obj = datetime.strptime(raw_datetime,dtformat) \n",
    "    return datetime_obj.hour\n",
    "\n",
    "def bizhour(hh):\n",
    "    \"\"\" Classifies the hour component of the datetime object.\n",
    "    Uses the following Classification:\n",
    "    Early: 5 AM to 10 AM\n",
    "    Business: 10 AM to 5 PM\n",
    "    Evening: 5 PM to 8 PM\n",
    "    Late: 8 PM to 5 AM\n",
    "    \"\"\"\n",
    "    biz = ['Late','Early','Business','Evening', 'Late']\n",
    "    breakpoints = [5, 10, 17,20]\n",
    "    return biz[bisect(breakpoints, hh)]\n",
    "\n",
    "def get_descr_bizhour(hhmat):\n",
    "    \"\"\" Maps the hour component to the dictionary created by the bizhour function.\n",
    "    \"\"\"\n",
    "    return np.array(list(map(bizhour, hhmat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Karthik's code\n",
    "class RepeatReplacer(object):\n",
    "    \"\"\" Removes repeating characters until a valid word is found.\n",
    "    >>> replacer = RepeatReplacer()\n",
    "    >>> replacer.replace(‘looooove’)\n",
    "    ‘love’\n",
    "    >>> replacer.replace(‘oooooh’)\n",
    "    ‘ooh’\n",
    "    >>> replacer.replace(‘goose’)\n",
    "    ‘goose’\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "    \n",
    "def process_tweets (tweets, textcol):\n",
    "    \"\"\"Takes text and performs the following functions:\n",
    "    - Replaces links with the word 'hyperlink'\n",
    "    - Replaces mentions with the word 'mention'\n",
    "    - Replaces pictures with the word 'image'\n",
    "    - Calls RepeatReplacer to remove repeat letters\n",
    "    - Stems the words using the Snoball Stemmer\n",
    "    \n",
    "    Returns a data frame with the same format as the input data frame, with the processed text instead.\n",
    "    \"\"\"\n",
    "    processed_text = []\n",
    "    for text in tweets[textcol]:\n",
    "        #replace hyperlinks - leaves xa0 off for some reason\n",
    "        test = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' hyperlink ', text).replace('\\xa0', '')\n",
    "        #replace mentions\n",
    "        test = re.sub(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)', ' mention ', test)\n",
    "        #replace picture links\n",
    "        test = re.sub(r'pic.twitter\\S+', ' image ', test)\n",
    "        test = \"\".join((char for char in test if char not in string.punctuation + '—–-…’0123456789')).lower()\\\n",
    "        #remove repeat letters\n",
    "        tokens = [RepeatReplacer().replace(w) for w in word_tokenize(test)]\n",
    "        #stem the words\n",
    "        tokens = [SnowballStemmer(\"english\").stem(w) for w in tokens]\n",
    "        #get rid of stop words\n",
    "        filtered_string = ' '.join([w for w in tokens if not w in set(stopwords.words('english'))])\n",
    "        \n",
    "        #add this to the text\n",
    "        processed_text.append(filtered_string)\n",
    "\n",
    "    tweets[textcol] = processed_text\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "def tfidf_vector (tweets, textcol):\n",
    "    \"\"\" From the input of the a data frame, creates an array of the text and implements TfidfVectorizer.\n",
    "    Returns the dense format of the TF-IDF counts, calculated across every word.\n",
    "    \"\"\"\n",
    "    tf = TfidfVectorizer(analyzer='word', min_df = 15, stop_words = 'english')\n",
    "    tfidf_matrix =  tf.fit_transform(tweets[textcol])\n",
    "    df = pd.DataFrame(tfidf_matrix.todense())\n",
    "    df.columns = tf.get_feature_names()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    \"\"\" Takes in a set of text. For each document in the text input, returns a negative, positive, and neutral sentiment score based on the VADER model in the nltk.\n",
    "    Returns the values for each as a set of arrays corresponding to the negative, neutral, and positive sentiments, respectively.\n",
    "    \"\"\"    \n",
    "    neg = []\n",
    "    neu = []\n",
    "    pos = []\n",
    "    for sentence in text:\n",
    "        vs = SentimentIntensityAnalyzer().polarity_scores(sentence)\n",
    "        neg.append(vs['neg'])\n",
    "        neu.append(vs['neu'])\n",
    "        pos.append(vs['pos'])\n",
    "    return neg, pos, neu\n",
    "\n",
    "def safe_div(x,y):\n",
    "    \"\"\" Returns zero if the divisor (y) is 0. Otherwise performs division\n",
    "    \"\"\"\n",
    "    if y == 0:\n",
    "        return 0\n",
    "    return x / y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsons_data = pd.DataFrame()\n",
    "\n",
    "directory = '/Users/jenniferpolson/Documents/School/2018-W/BE 223B/Project 1/tweet_files-1/'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.json'):\n",
    "        df = pd.DataFrame(json.load(open(directory + str(filename))))\n",
    "        df['tweet_id'] = df['user_record_id'].map(str) + '_' + df.index.astype(str)\n",
    "        jsons_data = jsons_data.append(df) \n",
    "\n",
    "#jsons_data['Negative Sentiment'], jsons_data['Positive Sentiment'], jsons_data['Neutral Sentiment'] = sentiment_analysis(jsons_data['text'])\n",
    "#jsons_data = jsons_data[jsons_data['Neutral Sentiment'] != 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out Tweets by Spam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12255, 8)\n",
      "(12965, 8)\n"
     ]
    }
   ],
   "source": [
    "df = jsons_data\n",
    "#remove shazam\n",
    "df = df[~df.text.str.contains(\"shazam\")]\n",
    "#remove one-word tweets\n",
    "df = df[df.text.str.contains(\" \")]\n",
    "print(df.shape)\n",
    "print(jsons_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        #for doc_index in top_doc_indices:\n",
    "        #    print (documents[doc_index])\n",
    "\n",
    "#create the documents that the LDA model can work off of\n",
    "documents = process_tweets(jsons_data, 'text').text\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run LDA\n",
    "lda_model = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_\n",
    "\n",
    "#n_top_words = 8\n",
    "#n_top_documents = 4\n",
    "#display_topics(lda_H, lda_W, tf_feature_names, documents, n_top_words, n_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create LDA column names\n",
    "cols = ['LDA_%d' % n for n in np.arange(no_topics)]\n",
    "\n",
    "#add the the topic model information to the tweet matrix\n",
    "all_data = pd.concat([jsons_data, pd.DataFrame(lda_W, index = jsons_data.index, columns = cols) ], axis = 1)\n",
    "\n",
    "#create empty dataframe to store concatenated tweet information\n",
    "tweets_concat = pd.DataFrame()\n",
    "\n",
    "for tid in all_data.user_record_id.unique():\n",
    "    df = all_data.loc[all_data['user_record_id'] == tid]\n",
    "    wk_list = df.weekday.tolist()\n",
    "    n_tweets = len(df.index)\n",
    "\n",
    "    hours = np.zeros(len(df.index))\n",
    "\n",
    "    for i,rd in enumerate(df['time']):\n",
    "        hours[i]=parse_datetime(rd,'%H:%M:%S')\n",
    "\n",
    "    timeofday = list(get_descr_bizhour(hours))\n",
    "\n",
    "    tweets_concat = tweets_concat.append({'n_tweets': n_tweets,\n",
    "                                          'text':df['text'].str.cat(sep=', '), \n",
    "                                          'likes':df.likes.astype(int).sum(), \n",
    "                                          'replies':df.replies.astype(int).sum(), \n",
    "                                          'retweets':df.retweets.astype(int).sum(), \n",
    "                                          'weekday_mean': df.weekday.astype(int).mean(),\n",
    "                                          'wkday_0': wk_list.count(0)/n_tweets,\n",
    "                                          'wkday_1': wk_list.count(1)/n_tweets,\n",
    "                                          'wkday_2': wk_list.count(2)/n_tweets,\n",
    "                                          'wkday_3': wk_list.count(3)/n_tweets,\n",
    "                                          'wkday_4': wk_list.count(4)/n_tweets,\n",
    "                                          'wkday_5': wk_list.count(5)/n_tweets,\n",
    "                                          'wkday_6': wk_list.count(6)/n_tweets,\n",
    "                                          'time_late': timeofday.count('Late')/n_tweets,\n",
    "                                          'time_early': timeofday.count('Early')/n_tweets,\n",
    "                                          'time_business': timeofday.count('Business')/n_tweets,\n",
    "                                          'time_evening': timeofday.count('Evening')/n_tweets,\n",
    "                                          'user_id':df.user_record_id.iloc[0]}, ignore_index = True)\n",
    "\n",
    "for topic in cols:\n",
    "    tweets_concat[topic] = all_data.groupby(['user_record_id'])[topic].mean().tolist()\n",
    "    \n",
    "tweets_concat.to_csv('tweets_concatenated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model_df (tweets, labels, \n",
    "                     textcol = 'text', idcol = 'user_id', \n",
    "                     repcol = 'replies', lcol = 'likes', retcol = 'retweets', \n",
    "                     bincol = 'variable'):\n",
    "    \"\"\" Requires a data frame containing the text to be processed (textcol), as well as other features.\n",
    "        - idcol: the column of the identification number for the cases\n",
    "        - repcol: name of column containing replies\n",
    "        - lcol: name of column containing number of likes\n",
    "        - retcol: name of column containing number of reteweets\n",
    "    \n",
    "    First, this function calls process_tweets to create clean text. \n",
    "    \n",
    "    It then calculates a few new features:\n",
    "        - The sentiment using sentiment_analysis function\n",
    "        - The ratio of replies to likes and retweets\n",
    "    Finally, it creates the binary label according to the bincol (meaning the variable):\n",
    "        - In this example, the value is set to 3 and up.\n",
    "    \"\"\"\n",
    "    df = process_tweets(tweets, textcol)\n",
    "    #generate tfidf, concatenate with processed tweets\n",
    "    new_features = pd.concat([df, tfidf_vector(df, textcol)], axis = 1)\n",
    "    new_features = df\n",
    "    new_features.index = new_features[idcol]\n",
    "    #generate sentiment\n",
    "    new_features['Negative Sentiment'], new_features['Positive Sentiment'], new_features['Neutral Sentiment'] = sentiment_analysis(tweets[textcol])\n",
    "    #match and merge with labels\n",
    "    full_data = pd.merge(new_features, labels, how='inner', on=None, left_on=None, right_on=None,\n",
    "                         left_index=True, right_index=True).drop([idcol], axis=1)\n",
    "\n",
    "    ratio = []\n",
    "    for index, row in full_data.iterrows():\n",
    "        div = safe_div(row[repcol], (row[lcol] + row[retcol]))\n",
    "        ratio.append(div)\n",
    "\n",
    "    full_data['ratio'] = ratio\n",
    "    #this gets rid of duplicate columns\n",
    "    full_data = full_data.loc[:,~full_data.columns.duplicated()]\n",
    "    #binarize\n",
    "    full_data['binary_label'] = (full_data[bincol] >= 3).astype(int)\n",
    "    #full_data = full_data.drop(full_data[full_data.variable == 3].index)\n",
    "    \n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"twitter-data-deidentified.csv\", index_col='record_id')\n",
    "full_data = create_model_df(tweets_concat, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#export everything thus far\n",
    "full_data.drop('text', axis = 1).to_csv('features_id_text.csv')\n",
    "#full_data.to_csv(\"full_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
