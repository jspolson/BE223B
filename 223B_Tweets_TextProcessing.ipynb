{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import (wordnet, stopwords)\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.linear_model import (LogisticRegression, LogisticRegressionCV)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_extraction.text import (TfidfVectorizer, CountVectorizer)\n",
    "from sklearn.metrics import (confusion_matrix, \n",
    "                             recall_score, \n",
    "                             f1_score, \n",
    "                             accuracy_score, \n",
    "                             precision_score,\n",
    "                             roc_curve, auc, roc_auc_score)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datetime import *\n",
    "from bisect import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Functions to parse the time\n",
    "def parse_datetime(raw_datetime, dtformat):\n",
    "    if len(raw_datetime)<5:\n",
    "        return np.nan, 365, np.nan, np.nan    \n",
    "    datetime_obj = datetime.strptime(raw_datetime,dtformat) \n",
    "    return datetime_obj.hour\n",
    "\n",
    "def bizhour(hh):\n",
    "    biz = ['Late','Early','Business','Evening', 'Late']\n",
    "    breakpoints = [5, 10, 17,20]\n",
    "    return biz[bisect(breakpoints, hh)]\n",
    "\n",
    "def get_descr_bizhour(hhmat):\n",
    "    return np.array(list(map(bizhour, hhmat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Karthik's code\n",
    "class RepeatReplacer(object):\n",
    "    \"\"\" Removes repeating characters until a valid word is found.\n",
    "    >>> replacer = RepeatReplacer()\n",
    "    >>> replacer.replace(‘looooove’)\n",
    "    ‘love’\n",
    "    >>> replacer.replace(‘oooooh’)\n",
    "    ‘ooh’\n",
    "    >>> replacer.replace(‘goose’)\n",
    "    ‘goose’\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.repeat_regexp = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "        self.repl = r'\\1\\2\\3'\n",
    "\n",
    "    def replace(self, word):\n",
    "        if wordnet.synsets(word):\n",
    "            return word\n",
    "\n",
    "        repl_word = self.repeat_regexp.sub(self.repl, word)\n",
    "\n",
    "        if repl_word != word:\n",
    "            return self.replace(repl_word)\n",
    "        else:\n",
    "            return repl_word\n",
    "    \n",
    "def process_tweets (tweets, textcol):\n",
    "    processed_text = []\n",
    "    for text in tweets[textcol]:\n",
    "        #replace hyperlinks - leaves xa0 off for some reason\n",
    "        test = re.sub(r'\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*', ' hyperlink ', text).replace('\\xa0', '')\n",
    "        #replace mentions\n",
    "        test = re.sub(r'(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z]+[A-Za-z0-9]+)', ' mention ', test)\n",
    "        #replace picture links\n",
    "        test = re.sub(r'pic.twitter\\S+', ' image ', test)\n",
    "        test = \"\".join((char for char in test if char not in string.punctuation + '—–-…’0123456789')).lower()\\\n",
    "        #remove repeat letters\n",
    "        tokens = [RepeatReplacer().replace(w) for w in word_tokenize(test)]\n",
    "        #stem the words\n",
    "        tokens = [SnowballStemmer(\"english\").stem(w) for w in tokens]\n",
    "        #get rid of stop words\n",
    "        filtered_string = ' '.join([w for w in tokens if not w in set(stopwords.words('english'))])\n",
    "        \n",
    "        #add this to the text\n",
    "        processed_text.append(filtered_string)\n",
    "\n",
    "    tweets[textcol] = processed_text\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "def tfidf_vector (tweets, textcol):\n",
    "    tf = TfidfVectorizer(analyzer='word', min_df = 15, stop_words = 'english')\n",
    "\n",
    "    tfidf_matrix =  tf.fit_transform(tweets[textcol])\n",
    "    feature_names = tf.get_feature_names() \n",
    "    dense = tfidf_matrix.todense()\n",
    "    df = pd.DataFrame(dense)\n",
    "    df.columns = feature_names\n",
    "    \n",
    "    return df\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    neg = []\n",
    "    neu = []\n",
    "    pos = []\n",
    "    for sentence in text:\n",
    "        vs = analyzer.polarity_scores(sentence)\n",
    "        neg.append(vs['neg'])\n",
    "        neu.append(vs['neu'])\n",
    "        pos.append(vs['pos'])\n",
    "    return neg, pos, neu\n",
    "\n",
    "def safe_div(x,y):\n",
    "    if y == 0:\n",
    "        return 0\n",
    "    return x / y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jsons_data = pd.DataFrame()\n",
    "\n",
    "directory = '/Users/jenniferpolson/Documents/School/2018-W/BE 223B/Project 1/tweet_files-1/'\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.json'):\n",
    "        df = pd.DataFrame(json.load(open(directory + str(filename))))\n",
    "        df['tweet_id'] = df['user_record_id'].map(str) + '_' + df.index.astype(str)\n",
    "        jsons_data = jsons_data.append(df) \n",
    "\n",
    "#jsons_data['Negative Sentiment'], jsons_data['Positive Sentiment'], jsons_data['Neutral Sentiment'] = sentiment_analysis(jsons_data['text'])\n",
    "#jsons_data = jsons_data[jsons_data['Neutral Sentiment'] != 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print (\"Topic %d:\" % (topic_idx))\n",
    "        print (\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        #for doc_index in top_doc_indices:\n",
    "        #    print (documents[doc_index])\n",
    "\n",
    "df = process_tweets(jsons_data, 'text')\n",
    "documents = df.text\n",
    "#documents = jsons_data.text\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(documents)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "no_topics = 20\n",
    "\n",
    "# Run LDA\n",
    "lda_model = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "lda_W = lda_model.transform(tf)\n",
    "lda_H = lda_model.components_\n",
    "\n",
    "#n_top_words = 8\n",
    "#n_top_documents = 4\n",
    "#display_topics(lda_H, lda_W, tf_feature_names, documents, n_top_words, n_top_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create LDA column names\n",
    "cols = ['LDA_%d' % n for n in np.arange(no_topics)]\n",
    "\n",
    "#add the the topic model information to the tweet matrix\n",
    "all_data = pd.concat([jsons_data, pd.DataFrame(lda_W, index = jsons_data.index, columns = cols) ], axis = 1)\n",
    "\n",
    "#create empty dataframe to store concatenated tweet information\n",
    "tweets_concat = pd.DataFrame()\n",
    "\n",
    "for tid in all_data.user_record_id.unique():\n",
    "    df = all_data.loc[all_data['user_record_id'] == tid]\n",
    "    wk_list = df.weekday.tolist()\n",
    "    n_tweets = len(df.index)\n",
    "\n",
    "    hours = np.zeros(len(df.index))\n",
    "\n",
    "    for i,rd in enumerate(df['time']):\n",
    "        hours[i]=parse_datetime(rd,'%H:%M:%S')\n",
    "\n",
    "    timeofday = list(get_descr_bizhour(hours))\n",
    "\n",
    "    tweets_concat = tweets_concat.append({'n_tweets': n_tweets,\n",
    "                                          'text':df['text'].str.cat(sep=', '), \n",
    "                                          'likes':df.likes.astype(int).sum(), \n",
    "                                          'replies':df.replies.astype(int).sum(), \n",
    "                                          'retweets':df.retweets.astype(int).sum(), \n",
    "                                          'weekday_mean': df.weekday.astype(int).mean(),\n",
    "                                          'wkday_0': wk_list.count(0)/n_tweets,\n",
    "                                          'wkday_1': wk_list.count(1)/n_tweets,\n",
    "                                          'wkday_2': wk_list.count(2)/n_tweets,\n",
    "                                          'wkday_3': wk_list.count(3)/n_tweets,\n",
    "                                          'wkday_4': wk_list.count(4)/n_tweets,\n",
    "                                          'wkday_5': wk_list.count(5)/n_tweets,\n",
    "                                          'wkday_6': wk_list.count(6)/n_tweets,\n",
    "                                          'time_late': timeofday.count('Late')/n_tweets,\n",
    "                                          'time_early': timeofday.count('Early')/n_tweets,\n",
    "                                          'time_business': timeofday.count('Business')/n_tweets,\n",
    "                                          'time_evening': timeofday.count('Evening')/n_tweets,\n",
    "                                          'user_id':df.user_record_id.iloc[0]}, ignore_index = True)\n",
    "\n",
    "for topic in cols:\n",
    "    tweets_concat[topic] = all_data.groupby(['user_record_id'])[topic].mean().tolist()\n",
    "    \n",
    "tweets_concat.to_csv('tweets_concatenated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_model_df (tweets, labels, \n",
    "                     textcol = 'text', idcol = 'user_id', \n",
    "                     repcol = 'replies', lcol = 'likes', retcol = 'retweets', \n",
    "                     bincol = 'variable'):\n",
    "    df = process_tweets(tweets, textcol)\n",
    "    #generate tfidf, concatenate with processed tweets\n",
    "    new_features = pd.concat([df, tfidf_vector(df, textcol)], axis = 1)\n",
    "    new_features = df\n",
    "    new_features.index = new_features[idcol]\n",
    "    #generate sentiment\n",
    "    new_features['Negative Sentiment'], new_features['Positive Sentiment'], new_features['Neutral Sentiment'] = sentiment_analysis(tweets[textcol])\n",
    "    #match and merge with labels\n",
    "    full_data = pd.merge(new_features, labels, how='inner', on=None, left_on=None, right_on=None,\n",
    "                         left_index=True, right_index=True).drop([idcol], axis=1)\n",
    "\n",
    "    ratio = []\n",
    "    for index, row in full_data.iterrows():\n",
    "        div = safe_div(row[repcol], (row[lcol] + row[retcol]))\n",
    "        ratio.append(div)\n",
    "\n",
    "    full_data['ratio'] = ratio\n",
    "    #this gets rid of duplicate columns\n",
    "    full_data = full_data.loc[:,~full_data.columns.duplicated()]\n",
    "    #binarize\n",
    "    full_data['binary_label'] = (full_data[bincol] >= 3).astype(int)\n",
    "    #full_data = full_data.drop(full_data[full_data.variable == 3].index)\n",
    "    \n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = pd.read_csv(\"twitter-data-deidentified.csv\", index_col='record_id')\n",
    "full_data = create_model_df(tweets_concat, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export everything thus far\n",
    "#full_data.drop('text', axis = 1).to_csv('features_id_text.csv')\n",
    "#full_data.to_csv(\"full_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
